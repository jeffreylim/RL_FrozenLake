{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "# Frozen Lake $4\\times4$ „Å® $8\\times8$\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- ### Monte-Carlo Control\n",
    "    \n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src='frozenlake.jpg' width=1000 height=50/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "# Policy Iteration (Exploring starts)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "q_{\\pi}(s, a) &= \\mathbf{E}[G_t \\mid S_t = s, A_t = a] \\\\\n",
    "&= \\mathbf{E}[\\sum_{k=o}^{\\infty} \\gamma^k R_{t+k+1}^a \\mid S_t = s, A_t = a] \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\\pi_{*} (a \\mid s) := \\underset{a\\in A}{\\operatorname{argmax}} q_{\\pi}(s, a)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary alert-info\">\n",
    "    \n",
    "## Non-slippery version\n",
    "    \n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT, DOWN, RIGHT, UP, TERMINAL = 0, 1, 2, 3, -1\n",
    "\n",
    "\n",
    "def print_policy(policy, dim):\n",
    "    for state, action in enumerate(policy):\n",
    "        if state == env.nS - 1:\n",
    "            print('G', end=' ')\n",
    "        elif action == LEFT:\n",
    "            print('<', end=' ')\n",
    "        elif action == DOWN:\n",
    "            print('v', end=' ')\n",
    "        elif action == RIGHT:\n",
    "            print('>', end=' ')\n",
    "        elif action == UP:\n",
    "            print('^', end=' ')\n",
    "        else:\n",
    "            print('H', end=' ')\n",
    "        if (state + 1) % dim is 0:\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_non_slippery(env, gamma, epsilon, max_num_episodes, max_moves_per_episodes):\n",
    "    \n",
    "    policy = np.random.randint(0, env.nA, size=(env.nS)).astype(np.int8)\n",
    "    \n",
    "    action_values = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    num_visits_per_state = np.full((env.nS, env.nA), np.finfo(np.float64).eps)\n",
    "    \n",
    "    def policy_evaluation(episodes):\n",
    "        action_value = 0.0\n",
    "        for counter, episode in enumerate(episodes):\n",
    "            reward = episode[3]\n",
    "            action_value += reward * np.power(gamma, counter)\n",
    "        return action_value\n",
    "    \n",
    "    def policy_improvement(episodes):\n",
    "        for state in range(env.nS):\n",
    "            policy[state] = np.argmax(action_values[state])\n",
    "    \n",
    "    env.reset()\n",
    "    print('Start:')\n",
    "    env.render()\n",
    "    \n",
    "    dim = 4\n",
    "    if env.nS == 64:\n",
    "        dim = 8\n",
    "    \n",
    "    print('\\nInitial random policy:\\n')\n",
    "    print_policy(policy, dim)\n",
    "    \n",
    "    for iteration in range(max_num_episodes):\n",
    "        \n",
    "        episodes = []\n",
    "        curr_state = env.reset()\n",
    "        exploring_starts = True\n",
    "        \n",
    "        for _ in range(max_moves_per_episodes):\n",
    "            if exploring_starts == True:\n",
    "                action = np.random.randint(0, env.nA)\n",
    "                curr_state = np.random.randint(0, env.nS)\n",
    "                exploring_starts = False\n",
    "            else:\n",
    "                action = policy[curr_state]\n",
    "            \n",
    "            ###\n",
    "            # In openAI FrozenLake, behaviour of env.P[state][action] and env.step(action) varies.\n",
    "            # example: State=6, Action=3(Up)\n",
    "            # env.P[State=6][Action=3] returns next_state=2, reward=0.0, done=False\n",
    "            # env.step(Action=3) at State=6 returns next_state=0, reward=0.0, done=False\n",
    "            # use env.P over env.step\n",
    "            #(next_state, reward, finished, _) = env.step(action)\n",
    "            (transition_probability, next_state, reward, finished) = env.P[curr_state][action][0]\n",
    "            episodes.append((curr_state, action, next_state, reward, finished))\n",
    "            \n",
    "            curr_state = next_state\n",
    "            if finished:\n",
    "                break\n",
    "                \n",
    "        is_first_visit = np.zeros((env.nS, env.nA), dtype=bool)\n",
    "        prev_action_values = action_values.copy()\n",
    "        \n",
    "        for counter, episode in enumerate(episodes):\n",
    "            (curr_state, action, next_state, reward, finished) = episode\n",
    "            if (is_first_visit[curr_state, action] == 0):\n",
    "                action_values[curr_state, action] += policy_evaluation(episodes[counter:])\n",
    "                num_visits_per_state[curr_state, action] += 1\n",
    "                is_first_visit[curr_state, action] = True\n",
    "                \n",
    "        delta = np.fabs(action_values - prev_action_values).max()\n",
    "        if delta < epsilon * (1 - gamma) / gamma and delta != 0:\n",
    "            break\n",
    "        \n",
    "        policy_improvement(episodes)\n",
    "    \n",
    "    action_values /= num_visits_per_state\n",
    "    print('Number of iterations: ', iteration + 1)\n",
    "    # print('Action values:')\n",
    "    # print(action_values)\n",
    "    print('\\nFinal policy:\\n')\n",
    "    print_policy(policy, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $4\\times4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "v ^ < < \n",
      "\n",
      "^ v ^ v \n",
      "\n",
      "^ < < v \n",
      "\n",
      "< ^ v G \n",
      "\n",
      "Number of iterations:  100000\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> > v < \n",
      "\n",
      "v < v < \n",
      "\n",
      "> v v < \n",
      "\n",
      "< > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "gamma = 0.999\n",
    "epsilon = 0.01\n",
    "max_num_episodes = 100000\n",
    "max_moves_per_episodes = 1000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    mc_control_non_slippery(env, gamma, epsilon, max_num_episodes, max_moves_per_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $8\\times8$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "v > ^ ^ ^ v > < \n",
      "\n",
      "> > v < < v > ^ \n",
      "\n",
      "> v v v > ^ v > \n",
      "\n",
      "> v < > < > > > \n",
      "\n",
      "v < v ^ < > < > \n",
      "\n",
      "^ > > > ^ v v ^ \n",
      "\n",
      "^ ^ < > v > > ^ \n",
      "\n",
      "< < v ^ ^ > > G \n",
      "\n",
      "Number of iterations:  500000\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> > v v < v < v \n",
      "\n",
      "^ v v < v v > v \n",
      "\n",
      "> > v < v > > v \n",
      "\n",
      "> > > > v < v v \n",
      "\n",
      "> ^ ^ < v v > v \n",
      "\n",
      "^ < < > > v < v \n",
      "\n",
      "^ < > ^ < v < v \n",
      "\n",
      "^ > ^ < > > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "gamma = 0.999\n",
    "epsilon = 0.01\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    mc_control_non_slippery(env, gamma, epsilon, max_num_episodes, max_moves_per_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "## Slippery when Wet\n",
    "\n",
    "<img src='Slippery_when_wet.jpg' width=250 height=5/>\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_slippery(env, gamma, epsilon, max_num_episodes, max_moves_per_episodes):\n",
    "    \n",
    "    policy = np.random.randint(0, env.nA, size=(env.nS)).astype(np.int8)\n",
    "    \n",
    "    action_values = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    num_visits_per_state = np.full((env.nS, env.nA), np.finfo(np.float64).eps)\n",
    "    \n",
    "    def policy_evaluation(episodes):\n",
    "        action_value = 0.0\n",
    "        for counter, episode in enumerate(episodes):\n",
    "            reward = episode[3]\n",
    "            action_value += reward * np.power(gamma, counter)\n",
    "        return action_value\n",
    "    \n",
    "    def policy_improvement(episodes):\n",
    "        for state in range(env.nS):\n",
    "            policy[state] = np.argmax(action_values[state])\n",
    "        \n",
    "    env.reset()\n",
    "    print('Start:')\n",
    "    env.render()\n",
    "    \n",
    "    dim = 4\n",
    "    if env.nS == 64:\n",
    "        dim = 8\n",
    "        \n",
    "    print('\\nInitial random policy:\\n')\n",
    "    print_policy(policy, dim)\n",
    "    \n",
    "    for iteration in range(max_num_episodes):\n",
    "        \n",
    "        episodes = []\n",
    "        curr_state = env.reset()\n",
    "        exploring_starts = True\n",
    "        \n",
    "        for _ in range(max_moves_per_episodes):\n",
    "            if exploring_starts == True:\n",
    "                action = np.random.randint(0, env.nA)\n",
    "                curr_state = np.random.randint(0, env.nS)\n",
    "                exploring_starts = False\n",
    "            else:\n",
    "                action = policy[curr_state]\n",
    "\n",
    "            ###\n",
    "            # In openAI FrozenLake, behaviour of env.P[state][action] and env.step(action) varies.\n",
    "            # example: State=6, Action=3(Up)\n",
    "            # env.P[State=6][Action=3] returns next_state=2, reward=0.0, done=False\n",
    "            # env.step(Action=3) at State=6 returns next_state=0, reward=0.0, done=False\n",
    "            # use env.P over env.step\n",
    "            #next_state, reward, finished, _ = env.step(action)\n",
    "            returns_list = env.P[curr_state][action]\n",
    "            (transition_probability, next_state, reward, finished) = returns_list[np.random.randint(0, len(returns_list))]\n",
    "            \n",
    "            # Enhanced policy improvements using intended action lookbacks.\n",
    "            # Keep track of previous states to estimate true current state.\n",
    "            if curr_state - 1 == next_state:\n",
    "                action = LEFT\n",
    "            elif curr_state + 1 == next_state:\n",
    "                action = RIGHT\n",
    "            elif curr_state + dim == next_state:\n",
    "                action = DOWN\n",
    "            elif curr_state - dim == next_state:\n",
    "                action = UP\n",
    "            episodes.append((curr_state, action, next_state, reward, finished))\n",
    "            \n",
    "            curr_state = next_state\n",
    "            if finished:\n",
    "                break\n",
    "        \n",
    "        is_first_visit = np.zeros((env.nS, env.nA), dtype=bool)\n",
    "        prev_action_values = action_values.copy()\n",
    "        \n",
    "        for counter, episode in enumerate(episodes):\n",
    "            (curr_state, action, next_state, reward, finished) = episode\n",
    "            if (is_first_visit[curr_state, action] == 0):\n",
    "                action_values[curr_state, action] += policy_evaluation(episodes[counter:])\n",
    "                num_visits_per_state[curr_state, action] += 1\n",
    "                is_first_visit[curr_state, action] = True\n",
    "                \n",
    "        delta = np.fabs(action_values - prev_action_values).max()\n",
    "        if delta < epsilon * (1 - gamma) / gamma and delta != 0:\n",
    "            break\n",
    "        \n",
    "        policy_improvement(episodes)\n",
    "    \n",
    "    action_values /= num_visits_per_state\n",
    "    print('Number of iterations: ', iteration + 1)\n",
    "    # print('Action values:')\n",
    "    # print(action_values)\n",
    "    print('\\nFinal policy:\\n')\n",
    "    print_policy(policy, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $4\\times4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "< > > > \n",
      "\n",
      "< < ^ < \n",
      "\n",
      "> ^ v > \n",
      "\n",
      "v < > G \n",
      "\n",
      "Number of iterations:  500000\n",
      "\n",
      "Final policy:\n",
      "\n",
      "v < v < \n",
      "\n",
      "v < v < \n",
      "\n",
      "> v v < \n",
      "\n",
      "< > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    mc_control_slippery(env, gamma, epsilon, max_num_episodes, max_moves_per_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $8\\times8$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "v < > > v v ^ > \n",
      "\n",
      "> < < < > v > ^ \n",
      "\n",
      "^ ^ > ^ > v ^ v \n",
      "\n",
      "^ > ^ < < < > ^ \n",
      "\n",
      "< > < > ^ > ^ ^ \n",
      "\n",
      "> v ^ < < ^ < v \n",
      "\n",
      "^ ^ v ^ ^ ^ < v \n",
      "\n",
      "^ v < > < > ^ G \n",
      "\n",
      "Number of iterations:  500000\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> > > > > > v v \n",
      "\n",
      "> > > > > > v v \n",
      "\n",
      "^ ^ ^ < > > > v \n",
      "\n",
      "^ ^ < > ^ < > v \n",
      "\n",
      "^ ^ < < > > > v \n",
      "\n",
      "^ < < > > v < v \n",
      "\n",
      "^ < > ^ < v < v \n",
      "\n",
      "^ < < < > > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True)\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    mc_control_slippery(env, gamma, epsilon, max_num_episodes, max_moves_per_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
