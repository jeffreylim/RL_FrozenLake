{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "# Frozen Lake $4\\times4$ „Å® $8\\times8$\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- ### Policy-Iteration\n",
    "    \n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='frozenlake.jpg' width=1000 height=50/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "## Non-slippery version\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Policy Iteration (slippery = False)\n",
    "\n",
    "---\n",
    "\n",
    "The transitional probabilities are deterministic in the unslippery version. We can simplify\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "q_{\\pi}(s, a) &:= \\sum_{\\forall{s'}} p_{ss'}^a ( R_{s'}^a + \\gamma \\sup_{\\forall{a'}} \\{ q_{\\pi}(s', a') \\} ) \\\\\n",
    "&= p_{ss'}^a ( R_{s'}^a + \\gamma \\sup_{\\forall{a'}} \\{ q_{\\pi}(s', a') \\} )\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma, epsilon, max_iterations):\n",
    "    \n",
    "    LEFT, DOWN, RIGHT, UP, TERMINAL = 0, 1, 2, 3, -1\n",
    "    \n",
    "    def policy_evaluation():\n",
    "        for state in range(env.nS - 1):\n",
    "            action = policy[state]\n",
    "            transition_probability, next_state, reward, done = env.P[state][action][0]\n",
    "            best_next_action_value = np.max(action_values[next_state])\n",
    "            action_values[state][action] = transition_probability * (reward + gamma * best_next_action_value)\n",
    "                    \n",
    "    def policy_improvement():\n",
    "        for state in range(env.nS - 1):\n",
    "            best_action_value = 0.0\n",
    "            for action in range(env.nA):\n",
    "                transition_probability, next_state, reward, done = env.P[state][action][0]\n",
    "                best_next_action_value = np.max(action_values[next_state])\n",
    "                if best_action_value < transition_probability * (reward + gamma * best_next_action_value):\n",
    "                    best_action_value = transition_probability * (reward + gamma * best_next_action_value)\n",
    "                    policy[state] = action\n",
    "\n",
    "    def print_policy(n):\n",
    "        idx = 0\n",
    "        for state, action in enumerate(policy):\n",
    "            idx += 1\n",
    "            if state == env.nS - 1:\n",
    "                print('G', end=' ')\n",
    "            elif action == LEFT:\n",
    "                print('<', end=' ')\n",
    "            elif action == DOWN:\n",
    "                print('v', end=' ')\n",
    "            elif action == RIGHT:\n",
    "                print('>', end=' ')\n",
    "            elif action == UP:\n",
    "                print('^', end=' ')\n",
    "            if idx % n is 0:\n",
    "                print('\\n')\n",
    "  \n",
    "    policy = np.random.randint(0, 4, size=(env.nS))\n",
    "    \n",
    "    action_values = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    env.reset()\n",
    "    print('Start:')\n",
    "    env.render()\n",
    "    \n",
    "    print('\\nInitial random policy:\\n')\n",
    "    if env.nS == 64:\n",
    "        print_policy(8)\n",
    "    else:\n",
    "        print_policy(4)\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        iteration += 1\n",
    "        prev_action_values = action_values.copy()\n",
    "        policy_evaluation()\n",
    "        delta = np.fabs(action_values - prev_action_values).max()\n",
    "        if delta < epsilon * (1 - gamma) / gamma and delta != 0:\n",
    "            break\n",
    "        if iteration >= max_iterations:\n",
    "            break\n",
    "        policy_improvement()\n",
    "\n",
    "    print('\\nNumber of Iterations: ', iteration)\n",
    "    print('Delta: ', delta)\n",
    "    print('\\nFinal policy:\\n')\n",
    "    if env.nS == 64:\n",
    "        print_policy(8)\n",
    "    else:\n",
    "        print_policy(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $4\\times4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "v ^ < < \n",
      "\n",
      "^ v ^ v \n",
      "\n",
      "^ < < v \n",
      "\n",
      "< ^ v G \n",
      "\n",
      "\n",
      "Number of Iterations:  10000\n",
      "Delta:  0.0\n",
      "\n",
      "Final policy:\n",
      "\n",
      "v > v < \n",
      "\n",
      "v v v v \n",
      "\n",
      "> v v v \n",
      "\n",
      "< > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "gamma = 0.999\n",
    "epsilon = 0.01\n",
    "max_iterations = 10000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    policy_iteration(env, gamma, epsilon, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $8\\times8$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "> v > < > v > < \n",
      "\n",
      "^ < > < v > > < \n",
      "\n",
      "^ ^ v v ^ > < > \n",
      "\n",
      "v v v ^ ^ v > v \n",
      "\n",
      "v < < v < < v ^ \n",
      "\n",
      "^ > v < > ^ ^ > \n",
      "\n",
      "v v v ^ < < v ^ \n",
      "\n",
      "< > < < v ^ v G \n",
      "\n",
      "\n",
      "Number of Iterations:  10000\n",
      "Delta:  0.0\n",
      "\n",
      "Final policy:\n",
      "\n",
      "v v v v v v v v \n",
      "\n",
      "v v v > v v v v \n",
      "\n",
      "v v v v v > v v \n",
      "\n",
      "> > > > v v v v \n",
      "\n",
      "> > ^ v v v > v \n",
      "\n",
      "v > v > > v ^ v \n",
      "\n",
      "v v > ^ < v v v \n",
      "\n",
      "> > ^ < > > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "gamma = 0.999\n",
    "epsilon = 0.01\n",
    "max_iterations = 10000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    policy_iteration(env, gamma, epsilon, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "## Slippery when Wet\n",
    "\n",
    "<img src='Slippery_when_wet.jpg' width=250 height=5/>\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "q_{\\pi}(s, a) &:= \\sum_{\\forall{s'}} p_{ss'}^a ( R_{s'}^a + \\gamma \\sup_{\\forall{a'}} \\{ q_{\\pi}(s', a') \\} )\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "---\n",
    " \n",
    " - $\\frac{1}{4}$ of the states in the $4x4$ grid are holes. Any randomized starting policy has a only a small probability of reaching the goal state (and ultimately collecting the reward of $1.0$) without falling into the holes.\n",
    "\n",
    "\n",
    " - Enhanced policy improvements using intended action lookbacks. Keep track of previous states to estimate true current state.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_slippery(env, gamma, epsilon, max_iterations):\n",
    "    \n",
    "    LEFT, DOWN, RIGHT, UP, TERMINAL = 0, 1, 2, 3, -1\n",
    "    \n",
    "    def policy_evaluation():\n",
    "        for state in range(env.nS - 1):\n",
    "            action = policy[state]\n",
    "            total_expectation_state_value = 0.0\n",
    "            observations = env.P[state][action]\n",
    "            for observation in observations:\n",
    "                transition_probability, next_state, reward, done = observation\n",
    "                best_next_action_value = np.max(action_values[next_state])\n",
    "                total_expectation_state_value += ((reward + gamma * best_next_action_value) * transition_probability)\n",
    "            action_values[state][action] = total_expectation_state_value\n",
    "    \n",
    "    def policy_improvement(n):\n",
    "        for state in range(env.nS - 1):\n",
    "            best_action, best_action_value = 1, 0.0\n",
    "            for action in range(env.nA):\n",
    "                observations = env.P[state][action]\n",
    "                action_value, action = 0.0, 0\n",
    "                for observation in observations:\n",
    "                    transition_probability, next_state, reward, done = observation\n",
    "                    if action_value < reward + gamma * transition_probability * np.max(action_values[next_state]):\n",
    "                        action_value = reward + gamma * transition_probability * np.max(action_values[next_state])\n",
    "                        if state - 1 == next_state:\n",
    "                            action = LEFT\n",
    "                        elif state + 1 == next_state:\n",
    "                            action = RIGHT\n",
    "                        elif state + n  == next_state:\n",
    "                            action = DOWN\n",
    "                        elif state - n == next_state:\n",
    "                            action = UP\n",
    "                if best_action_value < action_value:\n",
    "                    best_action_value = action_value\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "    \n",
    "\n",
    "    def print_policy(n):\n",
    "        idx = 0\n",
    "        for state, action in enumerate(policy):\n",
    "            idx += 1\n",
    "            if state == env.nS - 1:\n",
    "                print('G', end=' ')\n",
    "            elif action == LEFT:\n",
    "                print('<', end=' ')\n",
    "            elif action == DOWN:\n",
    "                print('v', end=' ')\n",
    "            elif action == RIGHT:\n",
    "                print('>', end=' ')\n",
    "            elif action == UP:\n",
    "                print('^', end=' ')\n",
    "            else:\n",
    "                print('H', end=' ')\n",
    "            if idx % n is 0:\n",
    "                print('\\n')\n",
    "  \n",
    "    policy = np.random.randint(0, 4, size=(env.nS))\n",
    "    \n",
    "    action_values = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    env.reset()\n",
    "    print('Start:')\n",
    "    env.render()\n",
    "    \n",
    "    print('\\nInitial random policy:\\n')\n",
    "    if env.nS == 64:\n",
    "        print_policy(8)\n",
    "    else:\n",
    "        print_policy(4)\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        iteration += 1\n",
    "        prev_action_values = action_values.copy()\n",
    "        policy_evaluation()\n",
    "        delta = np.fabs(action_values - prev_action_values).max()\n",
    "        if delta < epsilon * (1 - gamma) / gamma and delta != 0:\n",
    "            break\n",
    "        if iteration >= max_iterations:\n",
    "            break\n",
    "        if env.nS == 64:\n",
    "            policy_improvement(8)\n",
    "        else:\n",
    "            policy_improvement(4)\n",
    "\n",
    "    print('\\nNumber of Iterations: ', iteration)\n",
    "    print('Delta: ', delta)\n",
    "    print('\\nFinal policy:\\n')\n",
    "    if env.nS == 64:\n",
    "        print_policy(8)\n",
    "    else:\n",
    "        print_policy(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $4\\times4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "^ < < v \n",
      "\n",
      "^ > ^ ^ \n",
      "\n",
      "> v < > \n",
      "\n",
      "v ^ ^ G \n",
      "\n",
      "\n",
      "Number of Iterations:  38\n",
      "Delta:  9.746847311198348e-08\n",
      "\n",
      "Final policy:\n",
      "\n",
      "v > v < \n",
      "\n",
      "v v v v \n",
      "\n",
      "> v v v \n",
      "\n",
      "v > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "max_iterations = 100000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    policy_iteration_slippery(env, gamma, epsilon, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $8\\times8$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "^ v < ^ v v ^ > \n",
      "\n",
      "< < > > ^ ^ ^ ^ \n",
      "\n",
      "v ^ ^ < > > > < \n",
      "\n",
      "< ^ ^ v v v ^ < \n",
      "\n",
      "< v v > < v > > \n",
      "\n",
      "< ^ v ^ ^ v < ^ \n",
      "\n",
      "< ^ < > < ^ ^ v \n",
      "\n",
      "< v < < > < v G \n",
      "\n",
      "\n",
      "Number of Iterations:  129\n",
      "Delta:  9.596370285913647e-08\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> > > > > > v v \n",
      "\n",
      "^ ^ ^ > > > > v \n",
      "\n",
      "^ ^ ^ v ^ > > v \n",
      "\n",
      "^ ^ < < ^ v > v \n",
      "\n",
      "^ ^ < v > > > v \n",
      "\n",
      "^ v v > > v v v \n",
      "\n",
      "^ v > ^ v v v v \n",
      "\n",
      "^ < < v > > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True)\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "max_iterations = 100000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    policy_iteration_slippery(env, gamma, epsilon, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
