{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "# Frozen Lake $4\\times4$ „Å® $8\\times8$\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- ### Q-Learning\n",
    "    \n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src='frozenlake.jpg' width=1000 height=50/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "# $\\lambda = 0$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "q_{t+1}(s, a) &:= q_{t}(s, a) + \\eta (r^{a}_{t+1} + \\gamma \\sup_{\\forall a'} \\{ q_{t+1}(s_{t+1}, a') \\} - q_{t}(s, a) ) \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\\pi_{*} (a \\mid s) := \\underset{a\\in A}{\\operatorname{argmax}} q_{\\pi}(s, a)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary alert-info\">\n",
    "    \n",
    "## Non-slippery version\n",
    "    \n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT, DOWN, RIGHT, UP, TERMINAL = 0, 1, 2, 3, -1\n",
    "\n",
    "\n",
    "def print_policy(policy, dim):\n",
    "    for state, action in enumerate(policy):\n",
    "        if state == env.nS - 1:\n",
    "            print('G', end=' ')\n",
    "        elif action == LEFT:\n",
    "            print('<', end=' ')\n",
    "        elif action == DOWN:\n",
    "            print('v', end=' ')\n",
    "        elif action == RIGHT:\n",
    "            print('>', end=' ')\n",
    "        elif action == UP:\n",
    "            print('^', end=' ')\n",
    "        else:\n",
    "            print('H', end=' ')\n",
    "        if (state + 1) % dim is 0:\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_non_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes):\n",
    "    \n",
    "    policy = np.random.randint(0, env.nA, size=(env.nS)).astype(np.int8)\n",
    "    \n",
    "    action_values = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    eligible_traces = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    env.reset()\n",
    "    print('Start:')\n",
    "    env.render()\n",
    "    \n",
    "    dim = 4\n",
    "    if env.nS == 64:\n",
    "        dim = 8\n",
    "    \n",
    "    print('\\nInitial random policy:\\n')\n",
    "    print_policy(policy, dim)\n",
    "    \n",
    "    for iteration in range(max_num_episodes):\n",
    "        \n",
    "        #episodes = []\n",
    "        curr_state = env.reset()\n",
    "        exploring_starts = True\n",
    "        \n",
    "        for _ in range(max_moves_per_episodes):\n",
    "            if exploring_starts == True:\n",
    "                action = np.random.randint(0, env.nA)\n",
    "                curr_state = np.random.randint(0, env.nS)\n",
    "                exploring_starts = False\n",
    "            else:\n",
    "                action = policy[curr_state]\n",
    "            \n",
    "            prev_action_values = action_values.copy()\n",
    "            \n",
    "            ###\n",
    "            # In openAI FrozenLake, behaviour of env.P[state][action] and env.step(action) varies.\n",
    "            # example: State=6, Action=3(Up)\n",
    "            # env.P[State=6][Action=3] returns next_state=2, reward=0.0, done=False\n",
    "            # env.step(Action=3) at State=6 returns next_state=0, reward=0.0, done=False\n",
    "            # use env.P over env.step\n",
    "            #(next_state, reward, finished, _) = env.step(action)\n",
    "            (transition_probability, next_state, reward, finished) = env.P[curr_state][action][0]\n",
    "            \n",
    "            delta = reward + gamma * np.max(action_values[next_state])\n",
    "            action_values[curr_state, action] += eta * ( delta - action_values[curr_state, action] )\n",
    "            \n",
    "            policy[curr_state] = np.argmax(action_values[curr_state])\n",
    "            \n",
    "            curr_state = next_state\n",
    "            \n",
    "            if finished:\n",
    "                break\n",
    "                \n",
    "        delta = np.fabs(action_values - prev_action_values).max()\n",
    "        if delta < epsilon * (1 - gamma) / gamma and delta != 0:\n",
    "            break\n",
    "\n",
    "    print('Number of iterations: ', iteration + 1)\n",
    "    print('Delta: ', delta)\n",
    "    # print('Action values:')\n",
    "    # print(action_values)\n",
    "    print('\\nFinal policy:\\n')\n",
    "    print_policy(policy, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $4\\times4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "v ^ < < \n",
      "\n",
      "^ v ^ v \n",
      "\n",
      "^ < < v \n",
      "\n",
      "< ^ v G \n",
      "\n",
      "Number of iterations:  17022\n",
      "Delta:  1.0007350714769103e-07\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> > v < \n",
      "\n",
      "v < v < \n",
      "\n",
      "> v v < \n",
      "\n",
      "< > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "eta = 0.001\n",
    "max_num_episodes = 100000\n",
    "max_moves_per_episodes = 1000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    q_learning_non_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $8\\times8$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "^ < v > < > < > \n",
      "\n",
      "> > > ^ v < ^ v \n",
      "\n",
      "< > > > v < > > \n",
      "\n",
      "v ^ ^ ^ < < < < \n",
      "\n",
      "< > ^ > ^ ^ > < \n",
      "\n",
      "> v < > < < < > \n",
      "\n",
      "< ^ ^ ^ v ^ v < \n",
      "\n",
      "> < ^ ^ < v < G \n",
      "\n",
      "Number of iterations:  15299\n",
      "Delta:  1.0007350714769103e-07\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> v v v v v v v \n",
      "\n",
      "> > > > > v v v \n",
      "\n",
      "> v v < v > > v \n",
      "\n",
      "> > > > v < > v \n",
      "\n",
      "> ^ ^ < > > > v \n",
      "\n",
      "^ < < > ^ ^ < v \n",
      "\n",
      "^ < > ^ < v < v \n",
      "\n",
      "> > ^ < > > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "eta = 0.001\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    q_learning_non_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "## Slippery when Wet\n",
    "\n",
    "<img src='Slippery_when_wet.jpg' width=250 height=5/>\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes):\n",
    "    \n",
    "    policy = np.random.randint(0, env.nA, size=(env.nS)).astype(np.int8)\n",
    "    \n",
    "    action_values = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    eligible_traces = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    env.reset()\n",
    "    print('Start:')\n",
    "    env.render()\n",
    "    \n",
    "    dim = 4\n",
    "    if env.nS == 64:\n",
    "        dim = 8\n",
    "    \n",
    "    print('\\nInitial random policy:\\n')\n",
    "    print_policy(policy, dim)\n",
    "    \n",
    "    for iteration in range(max_num_episodes):\n",
    "        \n",
    "        curr_state = env.reset()\n",
    "        exploring_starts = True\n",
    "        \n",
    "        for _ in range(max_moves_per_episodes):\n",
    "            if exploring_starts == True:\n",
    "                action = np.random.randint(0, env.nA)\n",
    "                curr_state = np.random.randint(0, env.nS)\n",
    "                exploring_starts = False\n",
    "            else:\n",
    "                action = policy[curr_state]\n",
    "            \n",
    "            prev_action_values = action_values.copy()\n",
    "            \n",
    "            ###\n",
    "            # In openAI FrozenLake, behaviour of env.P[state][action] and env.step(action) varies.\n",
    "            # example: State=6, Action=3(Up)\n",
    "            # env.P[State=6][Action=3] returns next_state=2, reward=0.0, done=False\n",
    "            # env.step(Action=3) at State=6 returns next_state=0, reward=0.0, done=False\n",
    "            # use env.P over env.step\n",
    "            #(next_state, reward, finished, _) = env.step(action)\n",
    "            returns_list = env.P[curr_state][action]\n",
    "            (transition_probability, next_state, reward, finished) = returns_list[np.random.randint(0, len(returns_list))]\n",
    "            \n",
    "            # Enhanced policy improvements using intended action lookbacks.\n",
    "            # Keep track of previous states to estimate true current state.\n",
    "            if curr_state - 1 == next_state:\n",
    "                action = LEFT\n",
    "            elif curr_state + 1 == next_state:\n",
    "                action = RIGHT\n",
    "            elif curr_state + dim == next_state:\n",
    "                action = DOWN\n",
    "            elif curr_state - dim == next_state:\n",
    "                action = UP\n",
    "            \n",
    "            delta = reward + gamma * np.max(action_values[next_state])\n",
    "            action_values[curr_state, action] += eta * ( delta - action_values[curr_state, action] )\n",
    "            \n",
    "            policy[curr_state] = np.argmax(action_values[curr_state])\n",
    "            \n",
    "            curr_state = next_state\n",
    "            \n",
    "            if finished:\n",
    "                break\n",
    "                \n",
    "        delta = np.fabs(action_values - prev_action_values).max()\n",
    "        if delta < epsilon * (1 - gamma) / gamma and delta != 0:\n",
    "            break\n",
    "    \n",
    "    print('Number of iterations: ', iteration + 1)\n",
    "    print('Delta: ', delta)\n",
    "    # print('Action values:')\n",
    "    # print(action_values)\n",
    "    print('\\nFinal policy:\\n')\n",
    "    print_policy(policy, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $4\\times4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "^ > ^ > \n",
      "\n",
      "v > ^ v \n",
      "\n",
      "v ^ ^ > \n",
      "\n",
      "> < v G \n",
      "\n",
      "Number of iterations:  242\n",
      "Delta:  1.2959627868990897e-15\n",
      "\n",
      "Final policy:\n",
      "\n",
      "v > v < \n",
      "\n",
      "v < v < \n",
      "\n",
      "> > v < \n",
      "\n",
      "< > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "eta = 0.001\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    q_learning_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $8\\times8$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "< ^ ^ < < ^ ^ < \n",
      "\n",
      "^ v v < > v > > \n",
      "\n",
      "v < v v v < v ^ \n",
      "\n",
      "v < v < > < > > \n",
      "\n",
      "v > ^ > > < ^ > \n",
      "\n",
      "v > > < v v v > \n",
      "\n",
      "> > v < v v > < \n",
      "\n",
      "^ > v v > v < G \n",
      "\n",
      "Number of iterations:  1486\n",
      "Delta:  2.432959262407508e-13\n",
      "\n",
      "Final policy:\n",
      "\n",
      "v v v > > > v v \n",
      "\n",
      "> > > > > > v v \n",
      "\n",
      "^ > ^ < > > v v \n",
      "\n",
      "^ ^ ^ < v < v v \n",
      "\n",
      "> ^ ^ < > > > v \n",
      "\n",
      "^ < < > > v < v \n",
      "\n",
      "< < < ^ < v < v \n",
      "\n",
      "< < < < > > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True)\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "eta = 0.001\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "\n",
    "if __name__=='__main__':\n",
    "    q_learning_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
