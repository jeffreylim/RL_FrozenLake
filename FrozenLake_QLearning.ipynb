{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "# Frozen Lake $4\\times4$ „Å® $8\\times8$\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- ### Q-Learning\n",
    "    \n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src='frozenlake.jpg' width=1000 height=50/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import math\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "# $\\lambda = 0$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "q_{t+1}(s, a) &:= q_{t}(s, a) + \\eta (r^{a}_{t+1} + \\gamma \\max_{\\forall a'} \\{ q_{t+1}(s_{t+1}, a') \\} - q_{t}(s, a) ), \\eta \\in [0, 1], \\gamma \\in (0, 1] \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\\pi_{*} (a \\mid s) := \\underset{a\\in A}{\\operatorname{argmax}} q_{\\pi}(s, a)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary alert-info\">\n",
    "    \n",
    "## Non-slippery version\n",
    "    \n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT, DOWN, RIGHT, UP, TERMINAL = 0, 1, 2, 3, -1\n",
    "\n",
    "\n",
    "def print_policy(policy: np.ndarray, dim: int) -> None:\n",
    "    for state, action in enumerate(policy):\n",
    "        if state == env.nS - 1:\n",
    "            print('G', end=' ')\n",
    "        elif action == LEFT:\n",
    "            print('<', end=' ')\n",
    "        elif action == DOWN:\n",
    "            print('v', end=' ')\n",
    "        elif action == RIGHT:\n",
    "            print('>', end=' ')\n",
    "        elif action == UP:\n",
    "            print('^', end=' ')\n",
    "        else:\n",
    "            print('H', end=' ')\n",
    "        if (state + 1) % dim == 0:\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_non_slippery(env: typing.Type[gym.Wrapper],\n",
    "                            gamma: float,\n",
    "                            epsilon: float,\n",
    "                            eta: float,\n",
    "                            max_num_episodes: int,\n",
    "                            max_moves_per_episodes: int,\n",
    "                            max_exploring_actions: int) -> None:\n",
    "    \n",
    "    policy = np.random.randint(0, env.nA, size=(env.nS)).astype(np.int8)\n",
    "    action_values = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    env.reset()\n",
    "    print('Start:')\n",
    "    env.render()\n",
    "    \n",
    "    dim = np.sqrt(env.nS)\n",
    "    \n",
    "    print('\\nInitial random policy:\\n')\n",
    "    print_policy(policy, dim)\n",
    "    \n",
    "    for iteration in range(max_num_episodes):\n",
    "        \n",
    "        curr_state = env.reset()\n",
    "        \n",
    "        for _ in range(max_moves_per_episodes):\n",
    "            \n",
    "            if iteration < max_exploring_actions:\n",
    "                action = np.random.randint(0, env.nA)  # Explore action space\n",
    "            else:\n",
    "                action = policy[curr_state]\n",
    "            \n",
    "            prev_action_values = action_values.copy()\n",
    "            \n",
    "            next_state, reward, finished, _ = env.step(action)\n",
    "            \n",
    "            delta = reward + gamma * np.max(action_values[next_state])\n",
    "            action_values[curr_state, action] += eta * ( delta - action_values[curr_state, action] )\n",
    "            \n",
    "            policy[curr_state] = np.argmax(action_values[curr_state])\n",
    "            \n",
    "            curr_state = next_state\n",
    "            \n",
    "            if finished:\n",
    "                break\n",
    "                \n",
    "        delta = np.fabs(action_values - prev_action_values).max()\n",
    "        if delta <= epsilon * (1 - gamma) / gamma and not math.isclose(delta, 0.0):\n",
    "            break\n",
    "\n",
    "    print(f'Number of iterations: {iteration + 1}')\n",
    "    print(f'Delta: {delta}')\n",
    "    # print('Action values:\\n', action_values)\n",
    "    print('\\nFinal policy:\\n')\n",
    "    print_policy(policy, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $4\\times4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "v ^ < < \n",
      "\n",
      "^ v ^ v \n",
      "\n",
      "^ < < v \n",
      "\n",
      "< ^ v G \n",
      "\n",
      "Number of iterations: 10191\n",
      "Delta: 1.0007350714769103e-07\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> > v < \n",
      "\n",
      "v v v v \n",
      "\n",
      "> > v v \n",
      "\n",
      "< > > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "eta = 0.001\n",
    "max_num_episodes = 100000\n",
    "max_moves_per_episodes = 1000\n",
    "max_exploring_actions = 1000\n",
    "\n",
    "q_learning_non_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes, max_exploring_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $8\\times8$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "< v ^ < < > > ^ \n",
      "\n",
      "> > v > ^ ^ > ^ \n",
      "\n",
      "^ > ^ ^ ^ > ^ < \n",
      "\n",
      "< > v v > v < < \n",
      "\n",
      "^ < > ^ v < < < \n",
      "\n",
      "> v < v v ^ < ^ \n",
      "\n",
      "^ ^ v < v ^ < > \n",
      "\n",
      "> ^ > < < > ^ G \n",
      "\n",
      "Number of iterations: 4889\n",
      "Delta: 4.4353789258588883e-14\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> v v v > > > v \n",
      "\n",
      "> > > > > > > v \n",
      "\n",
      "^ ^ ^ ^ > > > v \n",
      "\n",
      "> > > > v v > v \n",
      "\n",
      "^ ^ ^ ^ > > > v \n",
      "\n",
      "^ v < > ^ ^ < v \n",
      "\n",
      "^ ^ > ^ v < < v \n",
      "\n",
      "^ < ^ < < < > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "eta = 0.001\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "max_exploring_actions = 10000\n",
    "\n",
    "q_learning_non_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes, max_exploring_actions=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "## Slippery when Wet\n",
    "\n",
    "<img src='Slippery_when_wet.jpg' width=250 height=5/>\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_slippery(env: typing.Type[gym.Wrapper],\n",
    "                        gamma: float,\n",
    "                        epsilon: float,\n",
    "                        eta: float,\n",
    "                        max_num_episodes: int,\n",
    "                        max_moves_per_episodes: int,\n",
    "                        max_exploring_actions: int) -> None:\n",
    "    \n",
    "    policy = np.random.randint(0, env.nA, size=(env.nS)).astype(np.int8)\n",
    "    action_values = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    env.reset()\n",
    "    print('Start:')\n",
    "    env.render()\n",
    "    \n",
    "    dim = np.sqrt(env.nS)\n",
    "    \n",
    "    print('\\nInitial random policy:\\n')\n",
    "    print_policy(policy, dim)\n",
    "    \n",
    "    for iteration in range(max_num_episodes):\n",
    "        \n",
    "        curr_state = env.reset()\n",
    "        \n",
    "        for _ in range(max_moves_per_episodes):\n",
    "            \n",
    "            if iteration < max_exploring_actions:\n",
    "                action = np.random.randint(0, env.nA) # Explore action space\n",
    "            else:\n",
    "                action = policy[curr_state]\n",
    "            \n",
    "            prev_action_values = action_values.copy()\n",
    "            \n",
    "            next_state, reward, finished, _ = env.step(action)\n",
    "            \n",
    "            # Enhanced policy improvements using intended action lookbacks.\n",
    "            # Keep track of previous states to estimate true current state.\n",
    "            if curr_state - 1 == next_state:\n",
    "                action = LEFT\n",
    "            elif curr_state + 1 == next_state:\n",
    "                action = RIGHT\n",
    "            elif curr_state + dim == next_state:\n",
    "                action = DOWN\n",
    "            elif curr_state - dim == next_state:\n",
    "                action = UP\n",
    "            \n",
    "            delta = reward + gamma * np.max(action_values[next_state])\n",
    "            action_values[curr_state, action] += eta * ( delta - action_values[curr_state, action] )\n",
    "            \n",
    "            policy[curr_state] = np.argmax(action_values[curr_state])\n",
    "            \n",
    "            curr_state = next_state\n",
    "            \n",
    "            if finished:\n",
    "                break\n",
    "                \n",
    "        delta = np.fabs(action_values - prev_action_values).max()\n",
    "        if delta <= epsilon * (1 - gamma) / gamma and not math.isclose(delta, 0.0):\n",
    "            break\n",
    "    \n",
    "    print(f'Number of iterations: {iteration + 1}')\n",
    "    print(f'Delta: {delta}')\n",
    "    # print('Action values:\\n', action_values)\n",
    "    print('\\nFinal policy:\\n')\n",
    "    print_policy(policy, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $4\\times4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "> v ^ < \n",
      "\n",
      "v > ^ > \n",
      "\n",
      "< > > < \n",
      "\n",
      "< v ^ G \n",
      "\n",
      "Number of iterations: 158\n",
      "Delta: 1.3903306411762718e-17\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> > v < \n",
      "\n",
      "v > v > \n",
      "\n",
      "> > v < \n",
      "\n",
      "< < > G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "eta = 0.001\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "max_exploring_actions = 1000\n",
    "\n",
    "q_learning_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes, max_exploring_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $8\\times8$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Initial random policy:\n",
      "\n",
      "v > < ^ ^ > > ^ \n",
      "\n",
      "v ^ < ^ ^ < v v \n",
      "\n",
      "< > v > > ^ > ^ \n",
      "\n",
      "^ < < ^ ^ > v v \n",
      "\n",
      "v v ^ < ^ v ^ > \n",
      "\n",
      "^ v v v < > < > \n",
      "\n",
      "> v < < < ^ < > \n",
      "\n",
      "^ v < > v < < G \n",
      "\n",
      "Number of iterations: 500000\n",
      "Delta: 0.0\n",
      "\n",
      "Final policy:\n",
      "\n",
      "> v v v v v v < \n",
      "\n",
      "> > > > > v v < \n",
      "\n",
      "> ^ ^ > > > v v \n",
      "\n",
      "> ^ ^ > v > v v \n",
      "\n",
      "> ^ ^ < > > > v \n",
      "\n",
      "^ v v > ^ ^ < v \n",
      "\n",
      "^ v > ^ < < < v \n",
      "\n",
      "^ < ^ > v < < G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True)\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 0.0001\n",
    "eta = 0.001\n",
    "max_num_episodes = 500000\n",
    "max_moves_per_episodes = 1000\n",
    "max_exploring_actions = 1000\n",
    "\n",
    "q_learning_slippery(env, gamma, epsilon, eta, max_num_episodes, max_moves_per_episodes, max_exploring_actions=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
